\section*{Question 1}

Let \( A, B \) be two events and \( I_{A}, I_{B} \) be the indicator random variables of these two events.
Let \( X=\max \left(I_{A}, I_{B}\right) \) and \( Y=\min \left(I_{A}, I_{B}\right) \).
Find the joint probability mass function of \( X, Y \) and \( \operatorname{Cov}(X, Y) \).

\subsection*{Solution}

We know that the indicator random variable \( I_{A} \) of event \( A \) is defined as
\begin{equation*}
    I_{A}=\begin{cases}
        1 & \text { if event } A \text { occurs } \\
        0 & \text { otherwise }
    \end{cases}
\end{equation*}
We can see that it takes the values \( 1 \) and \( 0 \) with probabilities \( P(A) \) and \( P(A^{c}) \) respectively.
From this, we can see that the range of the values of \( I_{A}, I_{B} \) is \( \{0, 1\} \), and hence the range of the values of \( X, Y \) is also \( \{0, 1\} \), thereby, \( X, Y \) are discrete random variables.

Now, we have
\begin{align*}
    \implies
    P(X=0, Y=0)
     & =
    P(I_{A}=0, I_{B}=0)
    =
    P(A^{c} \cap B^{c})
    \\
    \implies
    P(X=0, Y=1)
     & =
    0
    \\
    \implies
    P(X=1, Y=0)
     & =
    P(I_{A}=1, I_{B}=0) + P(I_{A}=0, I_{B}=1)
    \\ & =
    P(A \cap B^{c}) + P(A^{c} \cap B)
    \\
    \implies
    P(X=1, Y=1)
     & =
    P(I_{A}=1, I_{B}=1)
    =
    P(A \cap B)
\end{align*}

Therefore, the joint probability mass function of \( X, Y \) is
\begin{align*}
     &
    f_{XY}(x, y)
    =
    P(X=x, Y=y)
    \\
    \implies
     &
    \boxed{
        f_{XY}(x, y)
        =
        \begin{cases}
            P(A^{c} \cap B^{c})
             &
            \text { if } x=0, y=0
            \\
            P(A \cap B^{c}) + P(A^{c} \cap B)
             & \text { if } x=1, y=0
            \\
            P(A \cap B)
             & \text { if } x=1, y=1
            \\
            0
             & \text { otherwise }
        \end{cases}
    }
\end{align*}

To calculate the covariance of \( X, Y \), we have
\begin{equation*}
    \operatorname{Cov}(X, Y) = E[XY] - E[X]E[Y]
\end{equation*}

\begin{align*}
    \implies
    E[XY]
     & =
    \sum_{x} \sum_{y} xy f_{XY}(x, y)
    \\ & =
    \cancel{0 \cdot 0 \cdot f_{XY}(0, 0)}
    + \cancel{0 \cdot 1 \cdot f_{XY}(0, 1)}
    + \cancel{1 \cdot 0 \cdot f_{XY}(1, 0)}
    + 1 \cdot 1 \cdot f_{XY}(1, 1)
    \\ & =
    P(A \cap B)
\end{align*}

\begin{align*}
    \implies
    f_X (x)
     & =
    \sum_{y} f_{XY}(x, y)
    =
    f_{XY}(x, 0)
    + f_{XY}(x, 1)
    \\ & =
    \begin{cases}
        P(A^{c} \cap B^{c})
         &
        \text { if } x=0
        \\
        P(A \cap B^{c})
        + P(A^{c} \cap B)
        + P(A \cap B)
         &
        \text { if } x=1
    \end{cases}
    \\ & =
    \begin{cases}
        P(A^{c} \cap B^{c})
         &
        \text { if } x=0
        \\
        P(A \cup B)
         &
        \text { if } x=1
    \end{cases}
    \\
    \implies
    E[X]
     & =
    \sum_{x} x f_X(x)
    =
    0 \cdot f_X(0)
    + 1 \cdot f_X(1)
    =
    P(A \cup B)
\end{align*}

\begin{align*}
    \implies
    f_Y (y)
     & =
    \sum_{x} f_{XY}(x, y)
    =
    f_{XY}(0, y)
    + f_{XY}(1, y)
    \\ & =
    \begin{cases}
        P(A^{c} \cap B^{c})
        + P(A \cap B^{c})
        + P(A^{c} \cap B)
         &
        \text { if } y=0
        \\
        P(A \cap B)
         &
        \text { if } y=1
    \end{cases}
    \\ & =
    \begin{cases}
        P(A^{c} \cup B^{c})
         &
        \text { if } y=0
        \\
        P(A \cap B)
         &
        \text { if } y=1
    \end{cases}
    \\
    \implies
    E[Y]
     & =
    \sum_{y} y f_Y(y)
    =
    0 \cdot f_Y(0) + 1 \cdot f_Y(1)
    =
    P(A \cap B)
\end{align*}

Therefore, the covariance of \( X, Y \) is
\begin{align*}
    \implies
    \operatorname{Cov}(X, Y)
     & =
    E[XY] - E[X]E[Y]
    \\ & =
    P(A \cap B) - P(A \cup B) \cdot P(A \cap B)
    \\ & =
    \boxed{
        P(A \cap B) (1 - P(A \cup B))
    }
\end{align*}
